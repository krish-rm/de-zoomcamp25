{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('homework_module5') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "yellow_schema = types.StructType([\n",
    "    types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "    types.StructField(\"tpep_pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"tpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"passenger_count\", types.LongType(), True),  # Changed from IntegerType to LongType\n",
    "    types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "    types.StructField(\"RatecodeID\", types.LongType(), True),  # Changed from IntegerType to LongType\n",
    "    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"payment_type\", types.LongType(), True),  # Changed from IntegerType to LongType\n",
    "    types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"extra\", types.DoubleType(), True),\n",
    "    types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "    types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "    types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(yellow_schema) \\\n",
    "    .parquet('yellow_tripdata_2024-10.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.repartition(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.write.parquet('yellow_tripdata_2024-10_repartitioned.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size of each partitioned Parquet file: 24.08 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Path to the output Parquet directory\n",
    "output_dir = 'yellow_tripdata_2024-10_repartitioned.parquet'\n",
    "\n",
    "# Get a list of all Parquet files in the output directory (there will be 4 files)\n",
    "parquet_files = [f for f in os.listdir(output_dir) if f.endswith('.parquet')]\n",
    "\n",
    "# Get the size of each file in bytes\n",
    "file_sizes = [os.path.getsize(os.path.join(output_dir, f)) for f in parquet_files]\n",
    "\n",
    "# Convert the size to MB and calculate the average size\n",
    "total_size = sum(file_sizes)\n",
    "num_files = len(file_sizes)\n",
    "average_size_mb = total_size / num_files / (1024 * 1024)\n",
    "\n",
    "print(f\"Average size of each partitioned Parquet file: {average_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('tpep_pickup_date', F.to_date(df.tpep_pickup_datetime)) \\\n",
    "    .filter(\"tpep_pickup_date = '2024-10-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxi trips on 15th October 2024: 119103\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert tpep_pickup_datetime to a date format and filter for October 15, 2024\n",
    "oct_15_trips = df.filter(to_date(col(\"tpep_pickup_datetime\")) == \"2024-10-15\")\n",
    "\n",
    "# Count the number of trips that started on the 15th of October\n",
    "num_trips = oct_15_trips.count()\n",
    "\n",
    "print(f\"Number of taxi trips on 15th October 2024: {num_trips}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest trip is 162.62 hours.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "\n",
    "# Calculate the trip duration in seconds\n",
    "df_with_duration = df.withColumn(\"trip_duration_seconds\", \n",
    "                                 (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))))\n",
    "\n",
    "# Convert the trip duration from seconds to hours\n",
    "df_with_duration = df_with_duration.withColumn(\"trip_duration_hours\", col(\"trip_duration_seconds\") / 3600)\n",
    "\n",
    "# Find the longest trip duration in hours\n",
    "longest_trip_hours = df_with_duration.agg({\"trip_duration_hours\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"The length of the longest trip is {longest_trip_hours:.2f} hours.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
